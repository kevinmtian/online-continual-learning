Namespace(agent='ER', alpha=0.9, aser_type='asvm', batch=10, buffer_tracker=False, cl_type='ni', classifier_chill=0.01, clip=10.0, cuda=True, cumulative_delta=False, data='mini_imagenet', epoch=1, eps_mem_batch=10, error_analysis=False, fisher_update_after=50, fix_order=False, gss_batch_size=10, gss_mem_strength=10, head='mlp', k=5, kd_trick=False, kd_trick_star=False, labels_trick=False, lambda_=100, learning_rate=0.1, log_alpha=-300, mem_epoch=70, mem_iters=1, mem_size=5000, min_delta=0.0, minlr=0.0005, n_smp_cls=2.0, ncm_trick=False, ns_factor=(0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6), ns_task=(1, 1, 2, 2, 2, 2), ns_type='noise', num_runs=1, num_runs_val=3, num_tasks=10, num_val=3, online=True, optimizer='SGD', patience=0, plot_sample=False, retrieve='MIR', review_trick=False, save_path=None, seed=0, separated_softmax=False, stm_capacity=1000, store=False, subsample=50, temp=0.07, test_batch=128, update='random', val_size=0.1, verbose=True, warmup=4, weight_decay=0)
Setting up data stream
data setup time: 3.2363414764404297
0 0.0
1 0.4
2 0.8
3 1.2
4 1.6
5 2.0
6 2.4
7 2.8
8 3.2
9 3.6
buffer has 5000 slots
-----------run 0 training batch 0-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 6.254216, running train acc: 0.000
==>>> it: 1, mem avg. loss: 2.246610, running mem acc: 0.200
==>>> it: 101, avg. loss: 5.062602, running train acc: 0.020
==>>> it: 101, mem avg. loss: 3.170260, running mem acc: 0.290
==>>> it: 201, avg. loss: 4.864017, running train acc: 0.026
==>>> it: 201, mem avg. loss: 3.310176, running mem acc: 0.255
==>>> it: 301, avg. loss: 4.744853, running train acc: 0.032
==>>> it: 301, mem avg. loss: 3.361218, running mem acc: 0.238
==>>> it: 401, avg. loss: 4.664140, running train acc: 0.036
==>>> it: 401, mem avg. loss: 3.396904, running mem acc: 0.225
[0.073 0.081 0.076 0.066 0.063 0.059 0.048 0.052 0.063 0.05 ]
-----------run 0 training batch 1-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.510725, running train acc: 0.000
==>>> it: 1, mem avg. loss: 3.481429, running mem acc: 0.100
/data/tianmu/Projects/online-continual-learning/utils/buffer/reservoir_update.py:38: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  idx_new_data = valid_indices.nonzero().squeeze(-1)
==>>> it: 101, avg. loss: 4.214973, running train acc: 0.069
==>>> it: 101, mem avg. loss: 3.660965, running mem acc: 0.170
==>>> it: 201, avg. loss: 4.201505, running train acc: 0.074
==>>> it: 201, mem avg. loss: 3.617814, running mem acc: 0.168
==>>> it: 301, avg. loss: 4.178235, running train acc: 0.074
==>>> it: 301, mem avg. loss: 3.611560, running mem acc: 0.171
==>>> it: 401, avg. loss: 4.172417, running train acc: 0.075
==>>> it: 401, mem avg. loss: 3.576698, running mem acc: 0.178
[0.082 0.097 0.101 0.087 0.097 0.082 0.069 0.051 0.057 0.05 ]
-----------run 0 training batch 2-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.124333, running train acc: 0.050
==>>> it: 1, mem avg. loss: 3.355878, running mem acc: 0.250
==>>> it: 101, avg. loss: 4.097097, running train acc: 0.083
==>>> it: 101, mem avg. loss: 3.436037, running mem acc: 0.199
==>>> it: 201, avg. loss: 4.054690, running train acc: 0.093
==>>> it: 201, mem avg. loss: 3.405396, running mem acc: 0.209
==>>> it: 301, avg. loss: 4.016184, running train acc: 0.095
==>>> it: 301, mem avg. loss: 3.417847, running mem acc: 0.203
==>>> it: 401, avg. loss: 3.993259, running train acc: 0.097
==>>> it: 401, mem avg. loss: 3.399280, running mem acc: 0.204
[0.124 0.152 0.15  0.131 0.125 0.094 0.097 0.082 0.064 0.056]
-----------run 0 training batch 3-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.268321, running train acc: 0.050
==>>> it: 1, mem avg. loss: 2.682994, running mem acc: 0.350
==>>> it: 101, avg. loss: 3.912307, running train acc: 0.114
==>>> it: 101, mem avg. loss: 3.335973, running mem acc: 0.215
==>>> it: 201, avg. loss: 3.880287, running train acc: 0.116
==>>> it: 201, mem avg. loss: 3.326735, running mem acc: 0.225
==>>> it: 301, avg. loss: 3.867879, running train acc: 0.121
==>>> it: 301, mem avg. loss: 3.305793, running mem acc: 0.224
==>>> it: 401, avg. loss: 3.813460, running train acc: 0.129
==>>> it: 401, mem avg. loss: 3.289598, running mem acc: 0.229
[0.144 0.184 0.173 0.141 0.152 0.097 0.088 0.088 0.057 0.059]
-----------run 0 training batch 4-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.447514, running train acc: 0.300
==>>> it: 1, mem avg. loss: 2.856038, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.786181, running train acc: 0.141
==>>> it: 101, mem avg. loss: 3.044947, running mem acc: 0.254
==>>> it: 201, avg. loss: 3.755734, running train acc: 0.149
==>>> it: 201, mem avg. loss: 3.099653, running mem acc: 0.253
==>>> it: 301, avg. loss: 3.736207, running train acc: 0.151
==>>> it: 301, mem avg. loss: 3.097330, running mem acc: 0.253
==>>> it: 401, avg. loss: 3.723468, running train acc: 0.152
==>>> it: 401, mem avg. loss: 3.101593, running mem acc: 0.257
[0.185 0.199 0.189 0.167 0.178 0.161 0.128 0.103 0.077 0.058]
-----------run 0 training batch 5-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.485398, running train acc: 0.200
==>>> it: 1, mem avg. loss: 2.661661, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.602815, running train acc: 0.174
==>>> it: 101, mem avg. loss: 2.996511, running mem acc: 0.261
==>>> it: 201, avg. loss: 3.600809, running train acc: 0.175
==>>> it: 201, mem avg. loss: 3.003457, running mem acc: 0.264
==>>> it: 301, avg. loss: 3.602838, running train acc: 0.175
==>>> it: 301, mem avg. loss: 2.994469, running mem acc: 0.266
==>>> it: 401, avg. loss: 3.570219, running train acc: 0.181
==>>> it: 401, mem avg. loss: 3.007136, running mem acc: 0.268
[0.199 0.195 0.207 0.199 0.196 0.175 0.141 0.132 0.106 0.066]
-----------run 0 training batch 6-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.253568, running train acc: 0.200
==>>> it: 1, mem avg. loss: 3.131346, running mem acc: 0.150
==>>> it: 101, avg. loss: 3.576033, running train acc: 0.168
==>>> it: 101, mem avg. loss: 2.873861, running mem acc: 0.307
==>>> it: 201, avg. loss: 3.544197, running train acc: 0.182
==>>> it: 201, mem avg. loss: 2.875792, running mem acc: 0.303
==>>> it: 301, avg. loss: 3.523881, running train acc: 0.186
==>>> it: 301, mem avg. loss: 2.890820, running mem acc: 0.294
==>>> it: 401, avg. loss: 3.518435, running train acc: 0.184
==>>> it: 401, mem avg. loss: 2.887149, running mem acc: 0.296
[0.201 0.213 0.205 0.2   0.217 0.187 0.164 0.176 0.167 0.117]
-----------run 0 training batch 7-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.349043, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.985658, running mem acc: 0.350
==>>> it: 101, avg. loss: 3.502345, running train acc: 0.184
==>>> it: 101, mem avg. loss: 2.875254, running mem acc: 0.284
==>>> it: 201, avg. loss: 3.516909, running train acc: 0.188
==>>> it: 201, mem avg. loss: 2.868588, running mem acc: 0.296
==>>> it: 301, avg. loss: 3.496243, running train acc: 0.191
==>>> it: 301, mem avg. loss: 2.843021, running mem acc: 0.305
==>>> it: 401, avg. loss: 3.472391, running train acc: 0.196
==>>> it: 401, mem avg. loss: 2.842039, running mem acc: 0.302
[0.21  0.242 0.244 0.214 0.229 0.199 0.175 0.182 0.151 0.135]
-----------run 0 training batch 8-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 4.041502, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.974701, running mem acc: 0.250
==>>> it: 101, avg. loss: 3.424249, running train acc: 0.201
==>>> it: 101, mem avg. loss: 2.743234, running mem acc: 0.308
==>>> it: 201, avg. loss: 3.478017, running train acc: 0.196
==>>> it: 201, mem avg. loss: 2.693750, running mem acc: 0.320
==>>> it: 301, avg. loss: 3.439526, running train acc: 0.197
==>>> it: 301, mem avg. loss: 2.710678, running mem acc: 0.319
==>>> it: 401, avg. loss: 3.439841, running train acc: 0.197
==>>> it: 401, mem avg. loss: 2.679850, running mem acc: 0.330
[0.198 0.225 0.228 0.198 0.214 0.215 0.184 0.201 0.194 0.196]
-----------run 0 training batch 9-------------
size: (4500, 84, 84, 3), (4500,)
==>>> it: 1, avg. loss: 3.875836, running train acc: 0.100
==>>> it: 1, mem avg. loss: 2.294841, running mem acc: 0.350
==>>> it: 101, avg. loss: 3.440699, running train acc: 0.192
==>>> it: 101, mem avg. loss: 2.586936, running mem acc: 0.338
==>>> it: 201, avg. loss: 3.452850, running train acc: 0.191
==>>> it: 201, mem avg. loss: 2.626223, running mem acc: 0.334
==>>> it: 301, avg. loss: 3.465412, running train acc: 0.187
==>>> it: 301, mem avg. loss: 2.594203, running mem acc: 0.346
==>>> it: 401, avg. loss: 3.450907, running train acc: 0.191
==>>> it: 401, mem avg. loss: 2.579256, running mem acc: 0.350
[0.196 0.246 0.218 0.226 0.23  0.189 0.185 0.172 0.162 0.132]
-----------run 0-----------avg_end_acc 0.1956-----------train time 477.15481448173523
/data/tianmu/.conda/envs/mtian_env_cuda11/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: Degrees of freedom <= 0 for slice
  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,
/data/tianmu/.conda/envs/mtian_env_cuda11/lib/python3.8/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
----------- Total 1 run: 480.39132952690125s -----------
----------- Avg_End_Acc (0.1956, nan) Avg_End_Fgt (0.0191, nan) Avg_Acc (0.1655717857142857, nan) Avg_Bwtp (0.06331111111111115, nan) Avg_Fwt (0.09457777777777779, nan)-----------
